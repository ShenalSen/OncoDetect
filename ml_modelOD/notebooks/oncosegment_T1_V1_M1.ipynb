{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Breast Cancer Detection Model Development"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Install Required Libraries\n",
    "The required libraries for the project are installed using the `pip` package manager. These include:\n",
    "\n",
    "- **TensorFlow**: For deep learning and neural network implementation.\n",
    "- **Keras**: High-level API for building and training deep learning models.\n",
    "- **NumPy**: For numerical operations and array handling.\n",
    "- **Pandas**: For data manipulation and analysis.\n",
    "- **Scikit-learn**: For machine learning and evaluation metrics.\n",
    "- **Matplotlib**: For data visualization.\n",
    "- **OpenCV**: For image processing tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install tensorflow keras numpy pandas scikit-learn matplotlib opencv-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Import Libraries\n",
    "Essential libraries are imported to support data preprocessing, model creation, and visualization. \n",
    "- `EfficientNetB0` is used as the base model for transfer learning.\n",
    "- `ImageDataGenerator` helps with real-time data augmentation.\n",
    "- `Dense`, `Flatten`, and `Dropout` layers are used to create a custom classification head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.applications import EfficientNetB0 # type: ignore\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator # type: ignore\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout # type: ignore\n",
    "from tensorflow.keras.models import Model # type: ignore\n",
    "from tensorflow.keras.optimizers import Adam # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Load and Configure the Pretrained Model\n",
    "\n",
    "1. **EfficientNetB0 Pretrained Model**: Loaded with weights from the `ImageNet` dataset and configured to exclude the top layers for transfer learning.\n",
    "\n",
    "2. **Freezing Layers**: All layers in the base model are frozen to prevent updating their weights during training.\n",
    "\n",
    "3. **Custom Classification Head**: A new fully connected head is added to adapt the model for binary classification (breast cancer detection).\n",
    "   - `Flatten`: Flattens the output of the base model.\n",
    "   - `Dense(128)`: A dense layer with 128 neurons and ReLU activation.\n",
    "   - `Dropout(0.5)`: Dropout layer to prevent overfitting.\n",
    "   - `Dense(1, activation='sigmoid')`: Output layer with sigmoid activation for binary classification.\n",
    "   \n",
    "4. **Compilation**: The model is compiled with:\n",
    "   - **Adam optimizer**: For adaptive learning rate.\n",
    "   - **Binary cross-entropy loss**: For binary classification tasks.\n",
    "   - **Metrics**: Accuracy and AUC (Area Under the Curve)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = EfficientNetB0(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "x = base_model.output\n",
    "x = Flatten()(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "output = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=output)\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy', 'AUC'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Prepare the Dataset\n",
    "\n",
    "1. **ImageDataGenerator**: Used to augment data with transformations such as:\n",
    "   - Rescaling pixel values to the range `[0, 1]`.\n",
    "   - Rotation, width/height shift, zoom, and horizontal flip.\n",
    "\n",
    "2. **Training and Validation Split**: Data is split into training (80%) and validation (20%) subsets using the `validation_split` parameter.\n",
    "\n",
    "3. **Directory Configuration**: `flow_from_directory` is used to load images from the CBIS-DDSM dataset directory, with:\n",
    "   - **Target size**: Resizing images to `224x224`.\n",
    "   - **Batch size**: Defining batches of 32 images.\n",
    "   - **Class mode**: Binary classification for cancer detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "    rescale=1.0/255.0,\n",
    "    rotation_range=30,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    validation_split=0.2\n",
    ")\n",
    "\n",
    "train_data = datagen.flow_from_directory(\n",
    "    'path_to_cbis_ddsm',  # Replace with actual path\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='binary',\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "val_data = datagen.flow_from_directory(\n",
    "    'path_to_cbis_ddsm',  # Replace with actual path\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='binary',\n",
    "    subset='validation'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Train the Model\n",
    "\n",
    "1. **Model Fitting**: The model is trained using the `fit` function with:\n",
    "   - Training and validation datasets.\n",
    "   - 10 epochs for iterative learning.\n",
    "   - `steps_per_epoch` and `validation_steps` calculated based on batch sizes.\n",
    "\n",
    "2. **Output**: Training process returns a `history` object containing accuracy, loss, and AUC metrics for both training and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    train_data,\n",
    "    validation_data=val_data,\n",
    "    epochs=10,\n",
    "    steps_per_epoch=train_data.samples // train_data.batch_size,\n",
    "    validation_steps=val_data.samples // val_data.batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Evaluate and Save the Model\n",
    "\n",
    "1. **Model Evaluation**: The trained model is evaluated on the validation dataset to measure:\n",
    "   - **Validation Loss**: Quantifies error.\n",
    "   - **Validation Accuracy**: Proportion of correct predictions.\n",
    "   - **Validation AUC**: Indicates model's ability to distinguish between classes.\n",
    "   \n",
    "2. **Model Saving**: The trained model is saved as `oncodetect_model.h5` for future use or deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy, auc = model.evaluate(val_data)\n",
    "print(f'Validation Loss: {loss}, Accuracy: {accuracy}, AUC: {auc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Visualize Results\n",
    "\n",
    "1. **Accuracy Visualization**:\n",
    "   - Training and validation accuracy are plotted over epochs to monitor performance trends.\n",
    "   \n",
    "2. **AUC Visualization**:\n",
    "   - Training and validation AUC are plotted to evaluate classification effectiveness.\n",
    "\n",
    "The visualization helps identify potential issues such as overfitting or underfitting during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history['auc'], label='Training AUC')\n",
    "plt.plot(history.history['val_auc'], label='Validation AUC')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('AUC')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
